{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb09e937759475cf",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks with PyTorch on the MNIST Dataset\n",
    "\n",
    "In this live coding session, we will go through the process of building and training a simple neural network using PyTorch. We will be working with the MNIST dataset, a classic in the field of machine learning, which contains tens of thousands of handwritten digits.\n",
    "https://en.wikipedia.org/wiki/MNIST_database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c95024e145d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "# Run this cell to install the required packages if you haven't already.\n",
    "\n",
    "\n",
    "# !pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --extra-index-url https://download.pytorch.org/whl/cu117 \n",
    "# !pip install matplotlib seaborn scikit-learn torchview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc1efc1b0f40ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:22:11.404140Z",
     "start_time": "2024-05-01T09:22:11.393633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "# matplotlib.pyplot: Matplotlib module for creating static, animated, and interactive visualizations\n",
    "# numpy: NumPy is a library for the Python programming language, adding support for large, multidimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "# torch: PyTorch library for tensor computations and deep learning\n",
    "# torchvision: PyTorch library for computer vision tasks\n",
    "# transforms: torchvision module for common image transformations\n",
    "# nn, optim: PyTorch modules for neural networks and optimization algorithms\n",
    "# random_split: PyTorch module for splitting datasets\n",
    "# draw_graph: torchview module for visualizing neural network architectures\n",
    "\n",
    "# seaborn: Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "# confusion_matrix: sklearn module for computing confusion matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import random_split\n",
    "# from torchview import draw_graph\n",
    "\n",
    "# Checking for a GPU\n",
    "# torch.device: Returns a device object representing the device on which a torch.Tensor is or will be allocated.\n",
    "# torch.cuda.is_available: Returns a bool indicating if CUDA is currently available.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef2620-443b-4717-bbbc-9b29aaa747b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('haim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad1115b5623cfe2",
   "metadata": {},
   "source": [
    "# Loading the MNIST Dataset\n",
    "\n",
    "The MNIST dataset comes prepackaged with PyTorch's `torchvision` module. We'll download the dataset and set up `DataLoader` instances to batch and shuffle the data for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e50c555023d545f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:27:11.153409Z",
     "start_time": "2024-05-01T09:27:10.914773Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# MNIST Data Loaders\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Defining preprocessing steps for the dataset\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# transforms.ToTensor: Convert a PIL Image or numpy.ndarray to tensor of shape (C x H x W) in the range [0.0, 1.0] with type float instead of int with range [0, 255].\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# transforms.Normalize: Normalize a tensor image with mean and standard deviation. Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels, this transform will normalize each channel of the input torch.Tensor i.e. input[channel] = (input[channel] - mean[channel]) / std[channel]\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# It is used to normalize pixel values to be in the range of [-1, 1].\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      8\u001b[0m                                 transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Download and load the training data\u001b[39;00m\n\u001b[0;32m     11\u001b[0m train_set \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mMNIST(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "File \u001b[1;32m<stringsource>:69\u001b[0m, in \u001b[0;36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1429\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._line_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1487\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1906\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\PycharmProjects\\mnistProject\\.venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2197\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2194\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2196\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2197\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2199\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2202\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\mnistProject\\.venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2266\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2263\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[0;32m   2264\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[1;32m-> 2266\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2267\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m   2269\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MNIST Data Loaders\n",
    "\n",
    "# Defining preprocessing steps for the dataset\n",
    "# transforms.ToTensor: Convert a PIL Image or numpy.ndarray to tensor of shape (C x H x W) in the range [0.0, 1.0] with type float instead of int with range [0, 255].\n",
    "# transforms.Normalize: Normalize a tensor image with mean and standard deviation. Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels, this transform will normalize each channel of the input torch.Tensor i.e. input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "# It is used to normalize pixel values to be in the range of [-1, 1].\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "total_size = len(train_set)\n",
    "val_size = int(total_size * 0.2)  # 20% for validation\n",
    "train_size = total_size - val_size\n",
    "train_set, val_set = random_split(train_set, [train_size, val_size])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5380e98121f467",
   "metadata": {},
   "source": [
    "# Visualizing the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3738130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics of the dataset\n",
    "print(f'Training images: {len(train_set)}')\n",
    "print(f'Validation images: {len(val_set)}')\n",
    "print(f'Test images: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9823b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing some training images\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Print the shape of the images\n",
    "print(f'Image shape: {images.shape}')\n",
    "print(f'Label shape: {labels.shape}')\n",
    "\n",
    "# Print unique labels in the dataset\n",
    "print(f'Unique Labels: {labels.unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c91475eba73211",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:28:15.070285Z",
     "start_time": "2024-05-01T09:28:14.548914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    np_img = img.numpy()\n",
    "    plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3976fd799669ca8",
   "metadata": {},
   "source": [
    "# Defining the Neural Network\n",
    "\n",
    "The model is a simple feed-forward neural network with two hidden layers and an output layer. Here's a breakdown of the structure:\n",
    "\n",
    "1. **Input Layer**: The input to the model is a 28x28 pixel image, which is flattened into a 1D tensor of size 784 (28*28). This is done in the forward method of the model using the `view` function.\n",
    "\n",
    "2. **First Hidden Layer (fc1)**: This is a fully connected (Linear) layer that takes the 784-dimensional input and transforms it into a 128-dimensional tensor. This is done using a weight matrix of size [784, 128] and a bias vector of size [128]. The ReLU activation function is applied to the output of this layer.\n",
    "\n",
    "3. **Second Hidden Layer (fc2)**: This is another fully connected layer that takes the 128-dimensional output from the previous layer and transforms it into a 64-dimensional tensor. This is done using a weight matrix of size [128, 64] and a bias vector of size [64]. The ReLU activation function is applied to the output of this layer.\n",
    "\n",
    "4. **Output Layer (fc3)**: This is the final fully connected layer that takes the 64-dimensional output from the previous layer and transforms it into a 10-dimensional tensor. This is done using a weight matrix of size [64, 10] and a bias vector of size [10]. The output of this layer is the final output of the model, representing the logits for each of the 10 classes (digits 0-9). The softmax function is typically applied to these logits outside the model to obtain the probability distribution over the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3776b736b9b1fa58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:35:25.817708Z",
     "start_time": "2024-05-01T09:35:25.764422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Neural Network Definition\n",
    "# nn.Module: Base class for all neural network modules.\n",
    "# nn.Linear: Applies a linear transformation to the incoming data.\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)  # 28*28 is the size of MNIST images\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)  # There are 10 classes in MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the image\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net().to(device)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3396595091ab6e5f",
   "metadata": {},
   "source": [
    "# Defining a Convolutional Neural Network\n",
    "The Convolutional Neural Network (CNN) consists of the following layers:\n",
    "\n",
    "1. **Convolutional Layer (conv1)**: This is the first layer of the CNN. It uses a convolution operation on the input layer to create several smaller feature maps. The layer takes a single-channel image (grayscale image) as input and applies 32 filters, each of size 3x3. Padding is applied to keep the spatial dimensions the same.\n",
    "\n",
    "2. **Pooling Layer (pool)**: This layer is used to reduce the spatial dimensions of the input volume. It uses a 2x2 max pooling operation, which means it selects the maximum element from the feature map within the 2x2 window.\n",
    "\n",
    "3. **Convolutional Layer (conv2)**: This is the second convolutional layer, which takes the 32 feature maps from the previous layer as input and applies 64 filters, each of size 3x3. Padding is applied to keep the spatial dimensions the same.\n",
    "\n",
    "4. **Fully Connected Layer (fc1)**: After the second pooling layer, the feature maps are flattened into a single vector (1D tensor), which serves as input to this fully connected layer. This layer reduces the dimension from 64*7*7 to 128.\n",
    "\n",
    "5. **Fully Connected Layer (fc2)**: This is the output layer of the network. It takes the 128-dimensional vector from the previous layer and reduces it to a 10-dimensional vector. Each element of this vector represents the probability of a particular class (digits 0-9).\n",
    "\n",
    "The ReLU activation function is applied after each convolutional and fully connected layer except for the last one. This function introduces non-linearity into the model, allowing it to learn more complex patterns. The output of the final layer is typically passed through a softmax function to obtain the probability distribution over the classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fcd9d19",
   "metadata": {},
   "source": [
    "![image.png](https://upload.wikimedia.org/wikipedia/commons/9/90/CNN-filter-animation-1.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b1747529c9d0dbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:41:34.751581Z",
     "start_time": "2024-05-01T09:41:34.716728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "cnn = CNN().to(device)\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d14e155ab71bda",
   "metadata": {},
   "source": [
    "# Visualizing the Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2142fa4b28f79234",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:43:28.003256Z",
     "start_time": "2024-05-01T09:43:27.898115Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_graph = draw_graph(cnn, input_size=(64,1,28,28))\n",
    "# model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5dca8a8f268aaf",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "We will now train our model using the training data. We will run through the dataset multiple times, in \"epochs\", updating our weights each time to improve the model's performance.\n",
    "Before we train the model, we need to define a loss function and choose an optimizer. We'll use cross-entropy loss and the SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5703b9a2771d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:52:17.411480Z",
     "start_time": "2024-05-01T09:50:31.758181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.2964180574764808, Validation Loss: 0.08154023264812187\n",
      "Epoch 2, Training Loss: 0.060462538052971165, Validation Loss: 0.06025825133249956\n",
      "Epoch 3, Training Loss: 0.04087811127809497, Validation Loss: 0.05141582302229004\n",
      "Epoch 4, Training Loss: 0.03123888484458439, Validation Loss: 0.0390324742905424\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "# optimizer.zero_grad: Clears the gradients of all optimized torch.Tensor s.\n",
    "# loss.backward: Computes the gradient of current tensor w.r.t. graph leaves.\n",
    "# optimizer.step: Performs a single optimization step.\n",
    "\n",
    "epochs = 5\n",
    "train_losses = []  # Store losses here\n",
    "val_losses = []  # Store validation losses here\n",
    "model = cnn  # Select the model to train\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    # Iterate over batches.\n",
    "    for images, labels in train_loader:\n",
    "        # Loading batch to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients from previous batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass - make predictions and calculate loss\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass - compute gradient and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate average training loss for the epoch\n",
    "    epoch_loss = running_loss/len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    # Validation\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():  # No need to calculate gradients for validation, only for training\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            running_val_loss += val_loss.item()\n",
    "\n",
    "    # Calculate average validation loss for the epoch\n",
    "    epoch_val_loss = running_val_loss/len(val_loader)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Training Loss: {epoch_loss}, Validation Loss: {epoch_val_loss}')\n",
    "    \n",
    "# Plot the training and validation losses\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "\n",
    "# Add title, labels, and legend\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Draw the updated plot\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc754826c7a53a1",
   "metadata": {},
   "source": [
    "# Evaluating the Model\n",
    "\n",
    "Let's evaluate the performance of our trained model on the test dataset, which the model has not seen during training. We will measure the accuracy of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f7a71e81954eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T09:53:50.576240Z",
     "start_time": "2024-05-01T09:53:42.377003Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# torch.no_grad: Disables gradient calculation, useful for inference (when dont need Tensor.backward())\n",
    "# torch.max: Returns the maximum value of all elements in the input tensor.\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total}%')\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# Initialize the prediction and label lists(tensors)\n",
    "pred_list=torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "label_list=torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        classes = classes.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # Append batch prediction results\n",
    "        pred_list=torch.cat([pred_list, preds.view(-1).cpu()])\n",
    "        label_list=torch.cat([label_list, classes.view(-1).cpu()])\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat=confusion_matrix(label_list.numpy(), pred_list.numpy())\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=range(nb_classes), yticklabels=range(nb_classes))\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846717bd0244cf8",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We have successfully trained a simple neural network to recognize handwritten digits with PyTorch! There are many ways we could improve this model, such as adding more layers, using different activation functions, applying more sophisticated optimizers, or implementing learning rate schedules.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

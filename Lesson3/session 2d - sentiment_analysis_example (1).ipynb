{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Text classification with the torchtext library\n",
    "==============================================\n",
    "\n",
    "In this tutorial, we will show how to use the torchtext library to build\n",
    "the dataset for the text classification analysis. Users will have the\n",
    "flexibility to  \n",
    "\n",
    "> -   Access to the raw data as an iterator\n",
    "> -   Build data processing pipeline to convert the raw text strings\n",
    ">     into `torch.Tensor` that can be used to train the model\n",
    "> -   Shuffle and iterate the data with\n",
    ">     [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)\n",
    "\n",
    "Prerequisites\n",
    "-------------\n",
    "\n",
    "A recent 2.x version of the `portalocker` package needs to be installed\n",
    "prior to running the tutorial. For example, in the Colab environment,\n",
    "this can be done by adding the following line at the top of the script:\n",
    "\n",
    "``` {.sourceCode .bash}\n",
    "!pip install -U portalocker>=2.0.0`\n",
    "```\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "python.org/downloads"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access to the raw dataset iterators\n",
    "===================================\n",
    "\n",
    "The torchtext library provides a few raw dataset iterators, which yield\n",
    "the raw text strings. To access torchtext datasets, please install torchdata following\n",
    "instructions at <https://github.com/pytorch/data>.\n",
    "\n",
    "\n",
    "For example, the `AG_NEWS` dataset iterators yield\n",
    "the raw data as a tuple of label and text. There are 4 different labels representing 4 types of new articles:\n",
    "1. World\n",
    "2. Sports\n",
    "3. Business\n",
    "4. Tech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first item - \n",
      "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n",
      "\n",
      "second item - \n",
      "(3, 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "# Create an iterator for the AG_NEWS dataset (split=\"train\")\n",
    "train_iter = iter(AG_NEWS(split=\"train\"))\n",
    "\n",
    "# Retrieve the first item from the dataset\n",
    "first_item = next(train_iter)\n",
    "print(f\"First item - \\n{first_item}\")\n",
    "\n",
    "# Retrieve the second item from the dataset\n",
    "second_item = next(train_iter)\n",
    "print(f\"\\nSecond item - \\n{second_item}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data processing pipelines\n",
    "=================================\n",
    "\n",
    "We have revisited the very basic components of the torchtext library,\n",
    "including vocab, word vectors, tokenizer. Those are the basic data\n",
    "processing building blocks for raw text string.\n",
    "\n",
    "Here is an example for typical NLP data processing with tokenizer and\n",
    "vocabulary. The first step is to build a vocabulary with the raw\n",
    "training dataset. Here we use built in factory function\n",
    "[build\\_vocab\\_from\\_iterator]{.title-ref} which accepts iterator that\n",
    "yield list or iterator of tokens. Users can also pass any special\n",
    "symbols to be added to the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Calling the training set\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "\n",
    "# Initialize a basic english tokenizer (converts to lowercase & splits by space/punctuation)\n",
    "# Example tokenizing: \"Hello, my name is Tamir!\" --> [\"hello\", \"my\", \"name\", \"is\", \"tamir\"]\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "\n",
    "# Generator function that yields tokenized text from a data iterator.\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Build a look-up table of vocabulary (covert words/tokens to indices)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "# Set default index for unknown words\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exmple of indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence:\n",
      "['here', 'is', 'an', 'example']\n",
      "\n",
      "Example sentence indexing:\n",
      "[475, 21, 30, 5297]\n"
     ]
    }
   ],
   "source": [
    "example_sen = ['here', 'is', 'an', 'example']\n",
    "example_sen_indexing = vocab(['here', 'is', 'an', 'example'])\n",
    "\n",
    "print(f\"Example sentence:\\n{example_sen}\")\n",
    "print(f\"\\nExample sentence indexing:\\n{example_sen_indexing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a tokenizer and a vocabulary look-up table - now we can prepare the text processing pipeline, which will be used to process the raw data strings from the iterators.\n",
    "\n",
    "The **text pipeline** converts a text string into a list of integers based on the lookup table defined in the vocabulary.\n",
    "\n",
    "The **label pipeline** converts the label into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text (Indices):\n",
      "[431, 425, 1, 1605, 14838, 113, 66, 2, 848, 13, 27, 14, 27, 15, 50725, 3, 431, 374, 16, 9, 67507, 6, 52258, 3, 42, 4009, 783, 325, 1]\n",
      "\n",
      "Processed Label (Integer):\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Get the first item from the dataset\n",
    "\n",
    "first_item = next(iter(train_iter))\n",
    "\n",
    "# Apply the pipelines\n",
    "processed_text = text_pipeline(first_item[1])\n",
    "processed_label = label_pipeline(first_item[0])\n",
    "\n",
    "print(f\"Processed Text (Indices):\\n{processed_text}\")\n",
    "print(f\"\\nProcessed Label (Integer):\\n{processed_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data batch and iterator\n",
    "================================\n",
    "\n",
    "[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)\n",
    "is recommended for PyTorch users (a tutorial is\n",
    "[here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)).\n",
    "It works with a map-style dataset that implements the `getitem()` and\n",
    "`len()` protocols, and represents a map from indices/keys to data\n",
    "samples. It also works with an iterable dataset with the shuffle\n",
    "argument of `False`.\n",
    "\n",
    "Before sending to the model, `collate_fn` function works on a batch of\n",
    "samples generated from `DataLoader`. Its purpose is to customize how batches of data are formed. The input to `collate_fn` is a\n",
    "batch of data with the batch size in `DataLoader`, and `collate_fn`\n",
    "processes them according to the data processing pipelines declared\n",
    "previously. Pay attention here and make sure that `collate_fn` is\n",
    "declared as a top level def. This ensures that the function is available\n",
    "in each worker.\n",
    "\n",
    "In this example, the text entries in the original data batch input are\n",
    "packed into a list and concatenated as a single tensor for the input of\n",
    "`nn.EmbeddingBag`. The offset is a tensor of delimiters to represent the\n",
    "beginning index of the individual sequence in the text tensor. Label is\n",
    "a tensor saving the labels of individual text entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch:\n",
    "        # Send each label of the batch through the label pipeline\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        # Send each text of the batch through the text pipeline (and convert to tensor)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        # 'offsets' keeps track of the beginning index of each individual sequence in the text tensor.\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model\n",
    "================\n",
    "\n",
    "The model is composed of the\n",
    "[nn.EmbeddingBag](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag)\n",
    "layer plus a linear layer for the classification purpose.\n",
    "`nn.EmbeddingBag` with the default mode of \\\"mean\\\" computes the mean\n",
    "value of a \"bag\" of embeddings. Although the text entries here have\n",
    "different lengths, `nn.EmbeddingBag` module requires no padding here\n",
    "since the text lengths are saved in offsets.\n",
    "\n",
    "Additionally, since `nn.EmbeddingBag` accumulates the average across the\n",
    "embeddings on the fly, `nn.EmbeddingBag` can enhance the performance and\n",
    "memory efficiency to process a sequence of tensors.\n",
    "\n",
    "**in short** - `nn.EmbeddingBag` gets the embeddings of all the words in a sentence and averages them, giving us 1 embedding vector for each sentence that encompases the meaning of the sentence.\n",
    "\n",
    "![image](https://pytorch.org/tutorials/_static/img/text_sentiment_ngrams_model.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        fully_connected = self.fc(embedded)\n",
    "        return fully_connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate an instance\n",
    "====================\n",
    "\n",
    "The `AG_NEWS` dataset has four labels and therefore the number of\n",
    "classes is four.\n",
    "\n",
    "``` {.sourceCode .sh}\n",
    "1 : World\n",
    "2 : Sports\n",
    "3 : Business\n",
    "4 : Sci/Tec\n",
    "```\n",
    "\n",
    "`embed_dim` - We build a model with the embedding dimension of 64.\n",
    "\n",
    "`vocab_size` - The vocab size is equal to the length of the vocabulary instance.\n",
    "\n",
    "`num_class` - The number of classes is equal to the number of labels, which is 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split=\"train\")\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to train the model and evaluate results.\n",
    "=========================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        # Prevents exploding gradients by capping their magnitude\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset and run the model\n",
    "===================================\n",
    "\n",
    "Since the original `AG_NEWS` has no validation dataset, we split the training dataset into train/val sets with a split ratio of 0.95 (train) and\n",
    "0.05 (valid). Here we use\n",
    "[torch.utils.data.dataset.random\\_split](https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split)\n",
    "function in PyTorch core library.\n",
    "\n",
    "[CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss)\n",
    "criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in a single class. It is useful when training a multi-class classification problem.\n",
    "\n",
    "[SGD](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html)\n",
    "implements stochastic gradient descent method as the optimizer. The initial learning rate is set to 5.0.\n",
    "[StepLR](https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR)\n",
    "is used here to multiply the learning rate by *gamma* (<1) every N epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1782 batches | accuracy    0.685\n",
      "| epoch   1 |  1000/ 1782 batches | accuracy    0.854\n",
      "| epoch   1 |  1500/ 1782 batches | accuracy    0.876\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 29.94s | valid accuracy    0.890 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1782 batches | accuracy    0.898\n",
      "| epoch   2 |  1000/ 1782 batches | accuracy    0.900\n",
      "| epoch   2 |  1500/ 1782 batches | accuracy    0.901\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 29.44s | valid accuracy    0.894 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1782 batches | accuracy    0.913\n",
      "| epoch   3 |  1000/ 1782 batches | accuracy    0.913\n",
      "| epoch   3 |  1500/ 1782 batches | accuracy    0.914\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 34.91s | valid accuracy    0.903 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1782 batches | accuracy    0.924\n",
      "| epoch   4 |  1000/ 1782 batches | accuracy    0.922\n",
      "| epoch   4 |  1500/ 1782 batches | accuracy    0.922\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 35.86s | valid accuracy    0.907 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1782 batches | accuracy    0.930\n",
      "| epoch   5 |  1000/ 1782 batches | accuracy    0.930\n",
      "| epoch   5 |  1500/ 1782 batches | accuracy    0.928\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 36.28s | valid accuracy    0.904 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1782 batches | accuracy    0.942\n",
      "| epoch   6 |  1000/ 1782 batches | accuracy    0.944\n",
      "| epoch   6 |  1500/ 1782 batches | accuracy    0.942\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 38.80s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1782 batches | accuracy    0.944\n",
      "| epoch   7 |  1000/ 1782 batches | accuracy    0.944\n",
      "| epoch   7 |  1500/ 1782 batches | accuracy    0.943\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 37.36s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1782 batches | accuracy    0.947\n",
      "| epoch   8 |  1000/ 1782 batches | accuracy    0.944\n",
      "| epoch   8 |  1500/ 1782 batches | accuracy    0.945\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 38.42s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1782 batches | accuracy    0.946\n",
      "| epoch   9 |  1000/ 1782 batches | accuracy    0.945\n",
      "| epoch   9 |  1500/ 1782 batches | accuracy    0.947\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 37.46s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1782 batches | accuracy    0.946\n",
      "| epoch  10 |  1000/ 1782 batches | accuracy    0.946\n",
      "| epoch  10 |  1500/ 1782 batches | accuracy    0.946\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 39.23s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "# Define loss function, optimizer & learning rate\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "# Create iteratables for train set and test set\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "# Convert iterable-style datasets to map-style datasets to enable random sampling the maps inside the DataLoader\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "# calulate how many samples are 95% and split into train/val\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "# Create the dataloaders\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "best_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if best_accu is not None and best_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        best_accu = accu_val\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with test dataset\n",
    "====================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the results of the test dataset...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.911\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on a random news\n",
    "=====================\n",
    "\n",
    "Use the best model so far and test a golf news.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "# This example is sports new\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" % ag_news_label[predict(ex_text_str, text_pipeline)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
